{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "abc = \"0123456789ABEKMHOPCTYX\"\n",
    "\n",
    "class TextRecognitionDataset(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, phase, root='./data',  alphabet=abc, transform=None, preprocessing=None):\n",
    "        \"\"\"Constructor for class.\n",
    "        \n",
    "        Args:\n",
    "            - phase:         String: 'train' or 'test'\n",
    "            - root:          Dir with images\n",
    "            - alphabet:      String of chars required for predicting.\n",
    "            - transforms:    Transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "            - preprocessing: Transformation for Complex images with classical cv methods\n",
    "        \"\"\"\n",
    "        super(TextRecognitionDataset, self).__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        self.root = root\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "        self._parse_path()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "\n",
    "        image = self.all_images[index]   \n",
    "        text = self.image2label[image]\n",
    "\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            if image in self.simple:\n",
    "                image_path = os.path.join(self.image_path_simple, image)    # train simple phase\n",
    "            else:\n",
    "                image_path = os.path.join(self.image_path_complex, image)   # train complex phase\n",
    "        else: \n",
    "            image_path = os.path.join(self.image_path, image)               # test phase\n",
    "\n",
    "\n",
    "        # read images and make one channel (gray)\n",
    "        image  = cv2.imread(image_path).astype(np.float32) / 255.\n",
    "        # image = image.convert('L')\n",
    "\n",
    "\n",
    "        if self.preprocessing:\n",
    "            image = self.preprocessing(image)\n",
    "\n",
    "\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        if self.transform:\n",
    "            output = self.transform(output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image2label)\n",
    "    \n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "\n",
    "        Args:\n",
    "            - String of text.\n",
    "        Returns:\n",
    "            List of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        seq = [self.alphabet.find(c) + 1 for c in text]\n",
    "        \n",
    "        return seq\n",
    "\n",
    "\n",
    "    def _parse_path(self):\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            self.image_path_simple = os.path.join(self.root, self.phase, self.phase, 'simple')\n",
    "            self.image_path_complex = os.path.join(self.root, self.phase, self.phase, 'complex')\n",
    "\n",
    "            self.simple = os.listdir(self.image_path_simple)\n",
    "            self.complex = os.listdir(self.image_path_complex)\n",
    "\n",
    "            self.all_images = self.simple + self.complex \n",
    "            self.image2label = {label: label.split('.')[0].split('_')[-1] for label in self.all_images}\n",
    "\n",
    "        elif self.phase == 'test':\n",
    "            self.image_path = os.path.join(self.root, self.phase, 'result')\n",
    "            self.all_images = os.listdir(self.image_path)\n",
    "            self.image2label = {label: label.split('.')[0].split('_')[-1] for label in self.all_images}\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Wrong name for phase: choose \"train\" or \"test\"')  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing: Transformation for Complex images with classical cv methods\n",
    "def ada_thr(img):\n",
    "    \"\"\"Function for preprocessing images.\n",
    "    Args:\n",
    "        - img: numpy array.\n",
    "    Returns:\n",
    "        concatenated numpy array of images with different treshold methods\n",
    "    \"\"\"\n",
    "    ada_mean_thr = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,11,2)\n",
    "    ada_gaus_thr = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    result = np.concatenate([img[:, :, None], ada_mean_thr[:, :, None], ada_gaus_thr[:, :, None]], axis=2)\n",
    "    return result\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item):\n",
    "        \"\"\"Apply resizing.\n",
    "        Args: \n",
    "            - item: Dict with keys \"image\", \"seq\", \"seq_len\", \"text\".\n",
    "        Returns: \n",
    "            Dict with image resized to self.size.\n",
    "        \"\"\"\n",
    "        \n",
    "        interpolation = cv2.INTER_AREA if self.size[0] < item[\"image\"].shape[1] else cv2.INTER_LINEAR\n",
    "        item[\"image\"] = cv2.resize(item[\"image\"], self.size, interpolation=interpolation)\n",
    "        \n",
    "        return item\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "\n",
    "    Args:\n",
    "        - batch: List of dataset __getitem__ return values (dicts).\n",
    "    Returns:\n",
    "        Dict with same keys but values are either torch.Tensors of batched images or sequences or so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for item in batch:\n",
    "        images.append(torch.from_numpy(item[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(item[\"seq\"])\n",
    "        seq_lens.append(item[\"seq_len\"])\n",
    "        texts.append(item[\"text\"])\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "defualt_transform = Resize(size=(320, 64))\n",
    "\n",
    "\n",
    "def get_dataloaders(BATCH_SIZE=16, preprcoessing=False, transform=defualt_transform):\n",
    "    if preprcoessing:\n",
    "        preprocessing_fun = ada_thr\n",
    "    else:\n",
    "        preprocessing_fun = None\n",
    "\n",
    "    train_set = TextRecognitionDataset('train', preprocessing=preprocessing_fun, transform=transform)\n",
    "    test_set = TextRecognitionDataset('test', preprocessing=preprocessing_fun, transform=transform)\n",
    "\n",
    "    train = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "    test = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    train_dataloader, test_dataloader = get_dataloaders() \n",
    "        \n",
    "    for batch in train_dataloader:\n",
    "        print('TRAIN')\n",
    "        print('image', batch['image'].shape)\n",
    "        print('seq', batch['seq'])\n",
    "        print('seq_len', batch['seq_len'])\n",
    "        print('text', batch['text'])\n",
    "        print()\n",
    "        break\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        print('TEST')\n",
    "        print('image',batch['image'].shape)\n",
    "        print('seq', batch['seq'])\n",
    "        print('seq_len', batch['seq_len'])\n",
    "        print('tresh', batch['text'])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "image torch.Size([16, 3, 64, 320])\n",
      "seq tensor([12,  4,  4,  1, 13, 14,  4,  6, 13,  2,  2,  3, 17, 11,  2,  6, 10, 19,\n",
      "         6,  3,  4, 21, 12,  9,  7, 21,  4, 10,  7, 13, 19,  4,  6, 11,  5,  6,\n",
      "        10, 17, 18,  4,  6, 13,  2,  4,  5, 13, 17,  6,  1, 16,  3,  6,  6, 18,\n",
      "        13,  2, 10,  8, 20,  3,  5,  3, 16, 16,  4,  6, 17,  3, 10,  9, 16, 17,\n",
      "         2,  6,  3, 11,  3,  4,  1, 15, 22,  8,  6,  1, 13,  7,  1,  7, 12, 12,\n",
      "         2,  8,  9, 20, 10,  1,  7, 17, 22,  4,  6, 11,  1,  1,  2, 19, 19,  2,\n",
      "         8,  3, 15,  5,  1,  5, 22, 20,  2,  8,  9, 12,  5,  7,  4, 17, 21, 10,\n",
      "         9, 12,  4,  1,  3, 16, 12,  2,  8,  9], dtype=torch.int32)\n",
      "seq_len tensor([8, 9, 8, 8, 8, 8, 9, 8, 9, 9, 9, 8, 9, 9, 8, 9], dtype=torch.int32)\n",
      "text ['B330EK35', 'E112OA159', 'C523YB86', 'Y396EC35', 'A459OP35', 'E134EO50', 'H255PE197', 'T242HH35', 'O298HO152', 'A230MX750', 'E606BB178', 'T906OX35', 'A001CC172', 'M404XT178', 'B463OY98', 'B302HB178']\n",
      "\n",
      "TEST\n",
      "image torch.Size([16, 3, 64, 320])\n",
      "seq tensor([ 1,  2,  2,  1,  2,  1,  1,  2,  1,  1,  1,  2,  1,  1,  1,  1,  2,  1,\n",
      "         1,  1,  2,  2,  1,  1,  1,  3,  2,  1,  1,  1,  4,  2,  1,  1,  1,  5,\n",
      "         2,  1,  1,  1,  6,  2,  1,  1,  1,  7,  2,  1,  1,  1,  8,  2,  1,  1,\n",
      "         1,  9,  2,  1,  1,  1, 10,  2,  1,  1,  2], dtype=torch.int32)\n",
      "seq_len tensor([1, 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4], dtype=torch.int32)\n",
      "tresh ['0', '1', '10', '100', '1000', '10000', '10001', '10002', '10003', '10004', '10005', '10006', '10007', '10008', '10009', '1001']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julia\\miniconda3\\envs\\work\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Не найдена указанная процедура'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\julia\\miniconda3\\envs\\work\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\julia\\miniconda3\\envs\\work\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "!python utils/datasets/dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
